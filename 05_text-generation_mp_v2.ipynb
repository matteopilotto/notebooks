{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e98779c-17e8-496f-8242-6bff16bebe47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "552a30ba-f180-4722-a1b6-edbed8143570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_name = 'gpt2-xl'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15897984-089a-46e6-b30e-e21f316844b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (0.09)%</td>\n",
       "      <td>only (0.05)%</td>\n",
       "      <td>best (0.05)%</td>\n",
       "      <td>Transformers (0.04)%</td>\n",
       "      <td>ultimate (0.02)%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>popular (0.17)%</td>\n",
       "      <td>powerful (0.05)%</td>\n",
       "      <td>common (0.05)%</td>\n",
       "      <td>famous (0.04)%</td>\n",
       "      <td>successful (0.03)%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most popular</td>\n",
       "      <td>toy (0.11)%</td>\n",
       "      <td>toys (0.07)%</td>\n",
       "      <td>Transformers (0.07)%</td>\n",
       "      <td>of (0.05)%</td>\n",
       "      <td>and (0.04)%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most popular toy</td>\n",
       "      <td>line (0.34)%</td>\n",
       "      <td>in (0.18)%</td>\n",
       "      <td>of (0.12)%</td>\n",
       "      <td>brand (0.06)%</td>\n",
       "      <td>line (0.03)%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most popular toy line</td>\n",
       "      <td>in (0.46)%</td>\n",
       "      <td>of (0.15)%</td>\n",
       "      <td>, (0.05)%</td>\n",
       "      <td>on (0.04)%</td>\n",
       "      <td>ever (0.03)%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most popular toy line in</td>\n",
       "      <td>the (0.66)%</td>\n",
       "      <td>history (0.12)%</td>\n",
       "      <td>America (0.07)%</td>\n",
       "      <td>Japan (0.02)%</td>\n",
       "      <td>North (0.01)%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most popular toy line in the</td>\n",
       "      <td>world (0.69)%</td>\n",
       "      <td>United (0.05)%</td>\n",
       "      <td>history (0.04)%</td>\n",
       "      <td>US (0.04)%</td>\n",
       "      <td>U (0.02)%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>, (0.40)%</td>\n",
       "      <td>. (0.31)%</td>\n",
       "      <td>and (0.10)%</td>\n",
       "      <td>with (0.02)%</td>\n",
       "      <td>today (0.02)%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input          Choice 1  \\\n",
       "0                               Transformers are the      most (0.09)%   \n",
       "1                          Transformers are the most   popular (0.17)%   \n",
       "2                  Transformers are the most popular       toy (0.11)%   \n",
       "3              Transformers are the most popular toy      line (0.34)%   \n",
       "4         Transformers are the most popular toy line        in (0.46)%   \n",
       "5      Transformers are the most popular toy line in       the (0.66)%   \n",
       "6  Transformers are the most popular toy line in the     world (0.69)%   \n",
       "7  Transformers are the most popular toy line in ...         , (0.40)%   \n",
       "\n",
       "            Choice 2               Choice 3               Choice 4  \\\n",
       "0       only (0.05)%           best (0.05)%   Transformers (0.04)%   \n",
       "1   powerful (0.05)%         common (0.05)%         famous (0.04)%   \n",
       "2       toys (0.07)%   Transformers (0.07)%             of (0.05)%   \n",
       "3         in (0.18)%             of (0.12)%          brand (0.06)%   \n",
       "4         of (0.15)%              , (0.05)%             on (0.04)%   \n",
       "5    history (0.12)%        America (0.07)%          Japan (0.02)%   \n",
       "6     United (0.05)%        history (0.04)%             US (0.04)%   \n",
       "7          . (0.31)%            and (0.10)%           with (0.02)%   \n",
       "\n",
       "              Choice 5  \n",
       "0     ultimate (0.02)%  \n",
       "1   successful (0.03)%  \n",
       "2          and (0.04)%  \n",
       "3         line (0.03)%  \n",
       "4         ever (0.03)%  \n",
       "5        North (0.01)%  \n",
       "6            U (0.02)%  \n",
       "7        today (0.02)%  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_txt = 'Transformers are the'\n",
    "input_ids = tokenizer(input_txt, return_tensors='pt')['input_ids'].to(device)\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for step in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration['Input'] = tokenizer.decode(input_ids[0]) # grab the first item to avoid error\n",
    "        output = model(input_ids=input_ids) # predict input's and next token's logits\n",
    "        next_token_logits = output.logits[0, -1, :] # next token's logits\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1) # convert logits to softmax\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = f\"{tokenizer.decode(token_id)} ({token_prob:.2f})%\"\n",
    "            iteration[f'Choice {choice_idx+1}'] = token_choice\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, None, 0]], dim=-1)\n",
    "        \n",
    "        iterations.append(iteration)\n",
    "        \n",
    "display(pd.DataFrame(iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144c1ed-7cf1-401e-9072-7983e259d2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52c0587d-1dcf-4116-8534-dbf63b10e36c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "torch.Size([50257])\n",
      "torch.Size([50257])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([])\n",
      "tensor([[41762,   364,   389,   262,   749]], device='cuda:0')\n",
      "tensor([[41762,   364,   389,   262,   749]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)\n",
    "next_token_probs = model(input_ids).logits[0, -1, :].softmax(dim=-1)\n",
    "print(next_token_probs.shape)\n",
    "sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "print(sorted_ids.shape)\n",
    "print(sorted_ids[None, None, 0].shape)\n",
    "token_id = next_token_probs.argmax(dim=-1)\n",
    "print(token_id.shape)\n",
    "print(torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1))\n",
    "print(torch.cat([input_ids, sorted_ids[0, None, None]], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81353299-d47b-48ee-ad42-62509dd4f437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
